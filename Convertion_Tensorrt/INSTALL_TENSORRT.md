# TensorRT Installation Guide

## üöÄ **Quick Status: ONNX Export Working!**

‚úÖ **Good News**: Your ONNX export succeeded! The conversion pipeline is working.
‚ö†Ô∏è **Next Step**: Install TensorRT to build the optimized engine.

## üìã **TensorRT Installation Options**

### Option 1: Conda Installation (Easiest)
```bash
# Install TensorRT via conda (recommended)
conda install -c nvidia tensorrt

# Or with pip
pip install tensorrt
```

### Option 2: Manual Installation
```bash
# Download TensorRT from NVIDIA
# https://developer.nvidia.com/tensorrt

# For Ubuntu 20.04/22.04 with CUDA 11.8:
wget https://developer.download.nvidia.com/compute/machine-learning/tensorrt/8.6.1/tgz/TensorRT-8.6.1.6.Linux.x86_64-gnu.cuda-11.8.cudnn8.6.tar.gz

# Extract and set paths
tar -xzf TensorRT-8.6.1.6.Linux.x86_64-gnu.cuda-11.8.cudnn8.6.tar.gz
export PATH=$PATH:$(pwd)/TensorRT-8.6.1.6/bin
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$(pwd)/TensorRT-8.6.1.6/lib

# Test installation
trtexec --help
```

### Your Current Setup
If you have TensorRT installed at `/usr/src/tensorrt/bin/trtexec`, you can:

```bash
# Test trtexec
/usr/src/tensorrt/bin/trtexec --help

# Our build script will automatically detect this path
./build_accurate_tensorrt.sh
```

### Option 3: Docker (No Installation Required)
```bash
# Use NVIDIA's TensorRT container
docker run --gpus all -it --rm \
    -v ~/MatchAnything:/workspace \
    nvcr.io/nvidia/tensorrt:23.08-py3

# Inside container:
cd /workspace/Convertion_Tensorrt
trtexec --onnx=out/accurate_matchanything_roma.onnx \
        --saveEngine=out/accurate_matchanything_roma.plan \
        --fp16 --workspace=4096
```

## üß™ **Test Without TensorRT (ONNX Runtime)**

While installing TensorRT, you can test your ONNX model:

```bash
# Test with ONNX Runtime (should already be installed)
python3 test_onnx_inference.py \
    --onnx out/accurate_matchanything_roma.onnx \
    --image0 /path/to/image1.jpg \
    --image1 /path/to/image2.jpg
```

This will verify that:
- ‚úÖ ONNX model loads correctly
- ‚úÖ Input/output shapes are correct
- ‚úÖ Inference pipeline works
- ‚úÖ Ready for TensorRT optimization

## üîç **About the Small ONNX File**

The ONNX file is small because:
- **No checkpoint weights loaded** (architecture mismatch)
- **Contains mostly constants** from random initialization
- **Structure is correct** but weights are random

### To Get Real Weights:

1. **Inspect your checkpoint**:
```bash
python3 inspect_checkpoint.py /path/to/matchanything_roma.ckpt
```

2. **The checkpoint has CNN structure** (`matcher.model.encoder.cnn.*`)
3. **Our model expects DINOv2** (`encoder.dino.*`)

This suggests we need to either:
- Find a DINOv2-based checkpoint, or
- Adapt our model to match the CNN structure

## üéØ **Immediate Next Steps**

1. **Install TensorRT** (any option above)
2. **Test ONNX with ONNX Runtime**:
   ```bash
   python3 test_onnx_inference.py --image0 img1.jpg --image1 img2.jpg
   ```
3. **Build TensorRT engine**:
   ```bash
   trtexec --onnx=out/accurate_matchanything_roma.onnx \
           --saveEngine=out/accurate_matchanything_roma.plan \
           --fp16 --workspace=4096
   ```

## üèÜ **Current Status**

‚úÖ **ONNX conversion works**
‚úÖ **Dimension issues resolved**  
‚úÖ **Pipeline is functional**
‚ö†Ô∏è **Need TensorRT installation**
‚ö†Ô∏è **Need compatible checkpoint** (for real weights)

The hard part (getting the conversion to work) is done! Now it's just installation and optimization. üöÄ